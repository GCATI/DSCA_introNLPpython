{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data    Email                                        Description\n",
      "0   spam  FREE UNLIMITED HARDCORE PORN direct 2 your mob...\n",
      "1   spam  HOT LIVE FANTASIES call now 08707509020 Just 2...\n",
      "2   spam  SMS. ac JSco: Energy is high, but u may not kn...\n",
      "3   spam  You have been specially selected to receive a ...\n",
      "4   spam  Get the official ENGLAND poly ringtone or colo...\n",
      "5   spam  Last Chance! Claim ur £150 worth of discount v...\n",
      "6   spam  URGENT This is our 2nd attempt to contact U. Y...\n",
      "7   spam  100 dating service cal;l 09064012103 box334sk38ch\n",
      "8   spam  URGENT We are trying to contact you Last weeke...\n",
      "9   spam  U have a secret admirer. REVEAL who thinks U R...\n",
      "10   ham  Shopping lor. Them raining mah hard 2 leave or...\n",
      "11   ham  Me also da, i feel yesterday night  wait til 2...\n",
      "12   ham  Come to me, slave. Your doing it again ... Goi...\n",
      "13   ham  No need lar. Jus testing e phone card. Dunno n...\n",
      "14   ham  Oh ! A half hour is much longer in Syria than ...\n",
      "15   ham                        Ü still attending da talks?\n",
      "16   ham  Sounds like something that someone testing me ...\n",
      "17   ham                   That's very rude, you on campus?\n",
      "18   ham  Goodnight, sleep well da please take care pa. ...\n",
      "19   ham  Lol please do. Actually send a pic of yourself...\n",
      "Sample data   Email                                        Description\n",
      "0  spam  free unlimited hardcore porn direct your mobil...\n",
      "1  spam  hot live fantasies call now just per min ntt l...\n",
      "2  spam  sms ac jsco energy is high but u may not know ...\n",
      "3  spam  you have been specially selected to receive a ...\n",
      "4  spam  get the official england poly ringtone or colo...\n",
      "          ac    access  actually   admirer        ag     again       all  \\\n",
      "0   0.000000  0.182323  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "2   0.193467  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "3   0.000000  0.000000  0.000000  0.000000  0.247068  0.000000  0.000000   \n",
      "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "7   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "8   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "9   0.000000  0.000000  0.000000  0.231931  0.000000  0.000000  0.000000   \n",
      "10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "11  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "12  0.000000  0.000000  0.000000  0.000000  0.000000  0.216464  0.000000   \n",
      "13  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "14  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.175855   \n",
      "15  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "16  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "17  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "18  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "19  0.000000  0.000000  0.228432  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "        also       and       ans  ...       won      work     worth     would  \\\n",
      "0   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "1   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "2   0.000000  0.000000  0.193467  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "3   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "4   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "5   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.236957  0.000000   \n",
      "6   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "7   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "8   0.000000  0.000000  0.000000  ...  0.247468  0.000000  0.000000  0.000000   \n",
      "9   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "10  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "11  0.302479  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "12  0.000000  0.190275  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "13  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "14  0.000000  0.000000  0.000000  ...  0.000000  0.175855  0.000000  0.000000   \n",
      "15  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "16  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.360319   \n",
      "17  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "18  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "19  0.000000  0.200796  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "         wow       yer  yesterday       you      your  yourself  \n",
      "0   0.000000  0.000000   0.000000  0.000000  0.144614  0.000000  \n",
      "1   0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  \n",
      "2   0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  \n",
      "3   0.000000  0.000000   0.000000  0.166077  0.000000  0.000000  \n",
      "4   0.000000  0.203509   0.000000  0.000000  0.000000  0.000000  \n",
      "5   0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  \n",
      "6   0.000000  0.000000   0.227746  0.000000  0.205506  0.000000  \n",
      "7   0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  \n",
      "8   0.000000  0.000000   0.000000  0.166346  0.000000  0.000000  \n",
      "9   0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  \n",
      "10  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  \n",
      "11  0.000000  0.000000   0.265884  0.000000  0.000000  0.000000  \n",
      "12  0.000000  0.000000   0.000000  0.145505  0.343388  0.000000  \n",
      "13  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  \n",
      "14  0.175855  0.000000   0.000000  0.118208  0.000000  0.000000  \n",
      "15  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  \n",
      "16  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  \n",
      "17  0.000000  0.000000   0.000000  0.305112  0.000000  0.000000  \n",
      "18  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  \n",
      "19  0.000000  0.000000   0.000000  0.000000  0.000000  0.228432  \n",
      "\n",
      "[20 rows x 237 columns]\n",
      "Counts per Cluster (array([0, 1]), array([12,  8], dtype=int64))\n",
      "Number of documents clustered 20\n",
      "number of iterations: 2\n",
      "   Email                                        Description  clusterresult\n",
      "0   spam  free unlimited hardcore porn direct your mobil...              1\n",
      "1   spam  hot live fantasies call now just per min ntt l...              1\n",
      "2   spam  sms ac jsco energy is high but u may not know ...              0\n",
      "3   spam  you have been specially selected to receive a ...              1\n",
      "4   spam  get the official england poly ringtone or colo...              1\n",
      "5   spam  last chance claim ur worth of discount voucher...              1\n",
      "6   spam  urgent this is our attempt to contact u your p...              1\n",
      "7   spam                               dating service cal l              0\n",
      "8   spam  urgent we are trying to contact you last weeke...              1\n",
      "9   spam  u have a secret admirer reveal who thinks u r ...              1\n",
      "10   ham   shopping lor them raining mah hard leave orchard              0\n",
      "11   ham  me also da i feel yesterday night wait til nig...              0\n",
      "12   ham  come to me slave your doing it again going int...              0\n",
      "13   ham  no need lar jus testing e phone card dunno net...              0\n",
      "14   ham  oh a half hour is much longer in syria than ca...              0\n",
      "15   ham                         ü still attending da talks              0\n",
      "16   ham  sounds like something that someone testing me ...              0\n",
      "17   ham                       that very rude you on campus              0\n",
      "18   ham  goodnight sleep well da please take care pa pl...              0\n",
      "19   ham  lol please do actually send a pic of yourself ...              0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Oct 22 11:48:27 2019\n",
    "\n",
    "@author: s-minhas\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "import operator\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from wordcloud import WordCloud\n",
    "from nltk.collocations import BigramCollocationFinder \n",
    "from nltk.metrics import BigramAssocMeasures \n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "plt.rcParams.update({'font.size': 7})\n",
    "import operator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "raw_data = pd.read_csv(\"C:/IR Course/NLP_Intro/SMSSpamCollection.csv\",  encoding='iso-8859-1') \n",
    "\n",
    "def split_text_to_tokens(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def remove_punctuation_from_tokens(tokens):\n",
    "    #punctuation = ['(', ')', '?', ':', ';', ',', '.', '!', '/', '\"', \"'\"] \n",
    "    punctuation='!?,.:;\"\\')(_-'\n",
    "    text_without_punctuations = []\n",
    "    for entity in tokens:\n",
    "        newstring = \"\"\n",
    "        for char in entity:\n",
    "            if(char not in punctuation):\n",
    "                  newstring+= char\n",
    "        text_without_punctuations.append(newstring)\n",
    "    return text_without_punctuations\n",
    "  \n",
    "\n",
    "def remove_non_alphabetic_tokens(tokens):\n",
    "    alphabetic_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.isalpha():\n",
    "            alphabetic_tokens.append(token)\n",
    "    return alphabetic_tokens\n",
    "\n",
    "\n",
    "\n",
    "def remove_stopwords_from_tokens(tokens):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    return [each_token for each_token in tokens if each_token not in stop_words]\n",
    "\n",
    "\n",
    "def set_tokens_to_lowercase(tokens):\n",
    "    \n",
    "    return [each_token.lower() for each_token in tokens]\n",
    "\n",
    "def preprocess(pstr1):\n",
    "    \n",
    "     s=split_text_to_tokens(pstr1)\n",
    "     s=remove_non_alphabetic_tokens(s)\n",
    "     s=remove_punctuation_from_tokens(s)\n",
    "     s=set_tokens_to_lowercase(s)\n",
    "     return s\n",
    "\n",
    "\n",
    "def getsampledata(pdf, psamp):\n",
    "    types = ['spam', 'ham']\n",
    "    allsamples = pd.DataFrame()\n",
    "    for i in types:\n",
    "        data1 = pdf[pdf.Email == i]\n",
    "        rows = np.random.choice(data1.index.values, psamp)\n",
    "        sampled_data = pdf.loc[rows] \n",
    "        allsamples = allsamples.append(sampled_data, ignore_index=True)\n",
    "\n",
    "    return allsamples\n",
    "\n",
    "\n",
    "samp_data = getsampledata (raw_data, 10)\n",
    "\n",
    "\n",
    "\n",
    "print (\"Sample data\", samp_data)\n",
    "\n",
    "alltext = \" \"   \n",
    "\n",
    "for index, row in samp_data.iterrows():\n",
    "           row['Description']=' '.join(preprocess(row['Description']))\n",
    "           alltext = row['Description'] + alltext\n",
    "    \n",
    "print (\"Sample data\", samp_data.head())\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "samp_data_vectorised = vectorizer.fit_transform(samp_data['Description'])\n",
    "\n",
    "\n",
    "tdm = pd.DataFrame(samp_data_vectorised.toarray(), columns = vectorizer.get_feature_names())\n",
    "print (tdm)\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=2)\n",
    "clusters = km.fit(tdm)\n",
    "#Show counts per cluster number\n",
    "print(\"Counts per Cluster\", np.unique(clusters.labels_, return_counts=True))\n",
    "\n",
    "#Check same number of documents returned\n",
    "print(\"Number of documents clustered\", np.unique(clusters.labels_, return_counts=True)[1].sum())\n",
    "\n",
    "#Show number of iterations of K-means\n",
    "print(\"number of iterations: {0}\".format(clusters.n_iter_))\n",
    "#add the cluster number to each input iati record\n",
    "samp_data['clusterresult']=clusters.labels_\n",
    "print (samp_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
