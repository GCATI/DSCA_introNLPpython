{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in: spam 1364\n",
      "Number of tokens in: ham 735\n",
      "Number of tokens in text: 2099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 2100 Total Unique Words: 80 Type/Token Ratio: 0.038095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 7 of 7 matches:\n",
      "ed a < UKP > 2000 prize GUARANTEED . Call 09061790125 from landline . Claim 30\n",
      "lex Ltd. PO Box 1013 IG11 OJA Please call our customer service representative \n",
      "NLIMITED TEXT ? Camcorder ? Reply or call 08000930705 NOW URGENT ! ! Your 4* C\n",
      " Holiday or £5000 await collection . Call 09050090044 Now toClaim . SAE , TC s\n",
      "ther £1000 cash or £200 prize . Just call 09066361921 URGENT ! We are trying t\n",
      "u have won a £800 prize GUARANTEED . Call 09050003091 from land line . Claim C\n",
      ". Valid12hrs only HOT LIVE FANTASIES call now 08707509020 Just 20p per min NTT\n",
      "                                     bigram  freq\n",
      "2                      (prize, GUARANTEED.)     2\n",
      "3                       (GUARANTEED., Call)     2\n",
      "25                            (£1000, cash)     2\n",
      "0                          (Mobile, number)     1\n",
      "65                            (quiz,, text)     1\n",
      "74                           (2nd, attempt)     1\n",
      "73                     (16+., GBP1.50/week)     1\n",
      "72                             (3UZ., 16+.)     1\n",
      "71                              (M26, 3UZ.)     1\n",
      "70                               (84,, M26)     1\n",
      "69                               (BOX, 84,)     1\n",
      "68                  (now!T&Cs, WinnersClub)     1\n",
      "67                        (85222, now!T&Cs)     1\n",
      "66                             (text, PLAY)     1\n",
      "63                              (Win, £200)     1\n",
      "64                          (weekly, quiz,)     1\n",
      "76                             (top, prize)     1\n",
      "62                     (08715705022, Think)     1\n",
      "61   (http//www.gr8prizes.com, 08715705022)     1\n",
      "60         (80878, http//www.gr8prizes.com)     1\n",
      "59                          (wk.TXT, GREAT)     1\n",
      "58                          (every, wk.TXT)     1\n",
      "57                            (cash, every)     1\n",
      "56                             (£250, cash)     1\n",
      "55                             (8800,, PSP)     1\n",
      "54                           (Nokia, 8800,)     1\n",
      "75                             (weeks, top)     1\n",
      "78                           (£200, prize.)     1\n",
      "77                          (either, £1000)     1\n",
      "52                             (wkly, comp)     1\n",
      "..                                      ...   ...\n",
      "8                            (Valid, 12hrs)     1\n",
      "7                            (3030., Valid)     1\n",
      "6                            (Claim, 3030.)     1\n",
      "5                        (landline., Claim)     1\n",
      "4                       (Call, 09061790125)     1\n",
      "26                          (£5000, prize!)     1\n",
      "27                        (Video, Handset?)     1\n",
      "28                          (Handset?, 750)     1\n",
      "40                           (£5000, await)     1\n",
      "49                        (Max10mins, Free)     1\n",
      "48               (Cost£1.50/pm,, Max10mins)     1\n",
      "47                 (SK38xh,, Cost£1.50/pm,)     1\n",
      "46                    (Stockport,, SK38xh,)     1\n",
      "45                  (POBox334,, Stockport,)     1\n",
      "44                         (toClaim., SAE,)     1\n",
      "43                      (Call, 09050090044)     1\n",
      "42                      (collection., Call)     1\n",
      "41                     (await, collection.)     1\n",
      "39                           (Sol, Holiday)     1\n",
      "29                           (750, anytime)     1\n",
      "38                               (Del, Sol)     1\n",
      "37                             (Costa, Del)     1\n",
      "36                      (call, 08000930705)     1\n",
      "35                      (Camcorder?, Reply)     1\n",
      "34                      (TEXT?, Camcorder?)     1\n",
      "33                       (UNLIMITED, TEXT?)     1\n",
      "32                       (mins?, UNLIMITED)     1\n",
      "31                        (networks, mins?)     1\n",
      "30                      (anytime, networks)     1\n",
      "100                          (5WB, 0870..k)     1\n",
      "\n",
      "[101 rows x 2 columns]\n",
      "Word2Vec(vocab=127, size=50, alpha=0.025)\n",
      "['Free', 'entry', 'in', '2', 'a', 'to', 'win', 'Text', 'receive', 'txt', 'been', 'now', 'and', 'you', 'for', 'it', 'network', 'customer', 'have', 'selected', 'prize', 'To', 'claim', 'call', 'Claim', 'Valid', 'your', 'mobile', 'or', 'U', 'the', 'latest', 'with', 'Call', 'The', 'Mobile', 'FREE', 'on', 'send', '16+', 'Reply', '4', 'URGENT!', 'You', 'won', '1', 'week', 'our', 'Txt', 'message', '-', 'ur', 'will', 'be', 'Please', 'by', 'reply', 'not', 'We', 'free', 'is', 'now!', 'all', '', 'I', 'that', 'of', 'are', 'awarded', 'UR', 'new', 'service', 'as', 'guaranteed', '£1000', 'cash', 'Your', 'text', 'Get', 'PO', 'Box', '16', 'contact', 'draw', 'shows', '150ppm', '4*', '£2000', 'This', 'from', 'u', 'know', 'get', 'any', 'For', '&', 'per', 'STOP', 'Send', 'only', 'out', '500', 'can', 'just', '18', 'who', 'so', 'NOW', 'me', 'at', 'stop', 'has', 'Just', 'this', 'weekly', 'number', 'Nokia', 'phone', '1st', 'Holiday', '2nd', 'attempt', 'an', 'every', 'CALL', '£100', '8007']\n",
      "[-0.14993055  0.0738869   0.14314115 -0.08958331 -0.12930548 -0.23970784\n",
      " -0.04172972  0.05594721 -0.09968847 -0.12525715 -0.07168036  0.09682228\n",
      " -0.20237173  0.3180575  -0.19169863 -0.14829704 -0.27020332  0.00910229\n",
      "  0.11061054  0.04441061  0.14706996  0.12308072 -0.06858663  0.04184269\n",
      " -0.17311448  0.02712615 -0.01267271 -0.12590122 -0.1673473  -0.00713798\n",
      " -0.22362283  0.10779053  0.3813999  -0.20058195  0.2450355   0.06619368\n",
      " -0.08444659  0.09847901 -0.25180444  0.13078855 -0.08753134  0.09991188\n",
      "  0.1168765   0.06204071 -0.12337697 -0.08509744  0.22477697 -0.16158119\n",
      " -0.00287241 -0.20942308]\n",
      "Word2Vec(vocab=127, size=50, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s-minhas\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:289: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=470, size=50, alpha=0.025)\n",
      "['until', 'only', 'in', 'n', 'great', 'e', 'there', 'got', 'Ok', 'wif', 'u', 'U', 'dun', 'say', 'so', 'early', 'c', 'already', 'then', 'I', \"don't\", 'think', 'he', 'goes', 'to', 'around', 'here', 'my', 'is', 'not', 'like', 'with', 'me.', 'They', 'me', 'As', 'your', 'has', 'been', 'as', 'for', 'all', 'friends', \"I'm\", 'gonna', 'be', 'home', 'soon', 'and', 'i', 'want', 'talk', 'about', 'this', 'stuff', \"I've\", 'enough', 'today.', 'the', 'right', 'you', 'wont', 'take', 'help', 'will', 'You', 'have', 'a', 'at', 'A', 'Oh', 'watching', 'remember', 'how', '2', 'his', 'Yes', 'He', 'v', 'make', 'if', 'way', 'its', 'b', 'Is', 'that', 'going', 'try', 'So', 'ü', 'pay', 'first', 'Then', 'when', 'da', 'finish', 'lunch', 'go', 'down', 'lor.', '3', 'ur', 'no', 'can', 'meet', 'up', 'Just', 'eat', 'really', 'This', 'getting', 'Lol', 'always', 'Did', 'bus', '?', 'Are', 'an', 'left', 'over', 'dinner', 'Do', 'feel', 'Love', 'back', '&amp;', 'car', \"I'll\", 'let', 'know', 'room', 'What', 'it', \"that's\", 'still', 'were', 'sure', 'being', 'or', 'why', 'x', 'us', 'Yeah', 'was', 'had', 'out', 'she', 'that.', 'But', 'we', 'Not', 'doing', 'too', '', 'K', 'tell', 'anything', 'you.', 'of', 'just', 'look', 'msg', 'on', 'may', 'but', 'her', 'done', 'see', 'lor...', 'did', 'do', \"i'm\", 'trying', 'Pls', 'wanted', ',', 'need', 'you,', '...', 'most', 'love', 'sweet', 'YOU', 'hope', 'well', 'am', '&lt;#&gt;', 'No', 'get', \"can't\", 'could', 'ask', 'bit', \"didn't\", 'even', 'are', 'time', 'saw', 'half', 'tomorrow', 'morning', \"he's\", 'our', 'place', 'tonight', 'never', 'by', 'thought', 'it,', 'since', 'best', 'happy', 'sorry', 'more', 'what', 'now', 'Sorry,', 'call', 'later', 'Tell', 'where', 'Your', 'pick', 'home.', 'good', 'Its', 'Sorry', 'ok', 'come', 'now?', 'check', 'said', 'give', 'class', 'IM', 'AT', 'waiting', 'once', 'very', 'after', 'same', 'How', 'much', 'there.', 'hi', 'Yup', 'next', 'If', 'one', 'send', 'came', 'babe', 'another', 'late', 'means', 'any', 'y', 'buy', 'later.', 'work', 'abt', 'When', '-', 'Please', 'text', 'name', 'long', 'them', 'And', 'guess', 'something', 'says', 'life', 'lot', 'dear', 'Thanks', 'making', 'some', 'would', 'My', 'better', 'again', 'Dont', 'cos', 'new', 'Cos', 'special', 'Happy', 'She', '4', 'We', 'went', 'school', 'pls', 'Will', 'Ü', 'wat', 'Good', 'do.', 'sent', 'money', 'dont', 'R', 'ME', 'haf', \"It's\", 'him', 'Got', 'forgot', \"you're\", 'little', 'things', 'those', 'd', 'Gud', 'Can', 'ya', 'who', 'from', 'job', 'The', 'thk', 'Ok...', 'Ur', 'out.', 'without', 'tv', 'because', 'miss', 'day', 'Hi', 'which', 'also', 'free', 'liao...', 'coming', 'cant', '.', 'now.', 'Have', 'til', 'end', 'ok.', 'guys', '!', 'Haha', 'jus', 'people', 'keep', 'friend', 'It', 'stop', 'someone', 'able', 'every', 'Hope', 'hav', 'nice', 'Hey', ':)', '&lt;DECIMAL&gt;', 'dat', 'please', 'today', 'before', 'big', 'few', 'use', 'time.', 'called', 'run', 'than', 'Dear', 'Or', 'ill', 'Where', 'reach', 'That', 'told', 'into', 'face', 'watch', \"it's\", 'u.', 'everything', 'didnt', 'ready', 'night', 'care', 'da.', 'you?', 'other', 'week', \"Don't\", 'MY', 'Why', 'plan', 'smile', 'might', '1', 'it.', 'All', 'person', 'Ok.', 'last', 'im', 'r', 'hour', 'thats', 'phone', 'message', 'should', 'find', 'made', 'day.', 'they', 'number', 'Am', 'two', 'In', 'ever', '5', 'sleep', 'meeting', 'Well', 'Wat', 'wish', 'quite', 'minutes', 'leave', 'having', 'Was', 'actually', 'put', \"i've\", 'wanna', 'off', 'thing', 'den', 'mind', 'dis', 'tot', ':-)', 'wait', 'many', 'working', 'shit', 'heart', \"That's\", 'days', 'bad', 'lor', \"i'll\", 'IS', 'bring', 'Me', 'saying', 'wants', '*', 'makes', 'hear', 'guy', 'yet', 'wan', 'Now', 'till', 'THE', 'start', 'probably', 'between']\n",
      "[-0.34528685 -0.14342478  0.01703125 -0.00241888 -0.48028475 -0.14147148\n",
      "  0.04695125 -0.01519825  0.10677122 -0.16592093  0.09549238 -0.06712222\n",
      " -0.1265788   0.04994505 -0.02883419  0.23522346  0.11109117  0.05873304\n",
      " -0.01676709 -0.02010203 -0.07553343 -0.00058034 -0.18499999  0.00407955\n",
      " -0.295343    0.09997132 -0.20231304 -0.17245868 -0.04499406  0.22098166\n",
      " -0.2695704   0.02044099  0.07502331  0.14434963  0.02289235 -0.01569931\n",
      " -0.2031353   0.13088474 -0.24573867 -0.01616621 -0.05629604  0.16384071\n",
      " -0.05825345  0.02357728  0.06500766  0.02656893  0.35051006 -0.06499247\n",
      " -0.15804571 -0.09611326]\n",
      "Word2Vec(vocab=470, size=50, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s-minhas\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:307: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\s-minhas\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:315: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\s-minhas\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:316: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s-minhas\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:383: UserWarning: Tight layout not applied. The bottom and top margins cannot be made large enough to accommodate all axes decorations. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Sep 19 10:14:08 2019\n",
    "\n",
    "@author: s-minhas\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "import operator\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from wordcloud import WordCloud\n",
    "from nltk.collocations import BigramCollocationFinder \n",
    "from nltk.metrics import BigramAssocMeasures \n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "plt.rcParams.update({'font.size': 7})\n",
    "import operator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#wd = r'C:/IR Course/NLP_Intro_Course-master/spamham'\n",
    "\n",
    "\n",
    "def split_text_to_tokens(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def remove_punctuation_from_tokens(tokens):\n",
    "    #punctuation = ['(', ')', '?', ':', ';', ',', '.', '!', '/', '\"', \"'\"] \n",
    "    punctuation='!?,.:;\"\\')(_-'\n",
    "    text_without_punctuations = []\n",
    "    for entity in tokens:\n",
    "        newstring = \"\"\n",
    "        for char in entity:\n",
    "            if(char not in punctuation):\n",
    "                  newstring+= char\n",
    "        text_without_punctuations.append(newstring)\n",
    "    return text_without_punctuations\n",
    "  \n",
    "\n",
    "def remove_non_alphabetic_tokens(tokens):\n",
    "    alphabetic_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.isalpha():\n",
    "            alphabetic_tokens.append(token)\n",
    "    return alphabetic_tokens\n",
    "\n",
    "\n",
    "\n",
    "def remove_stopwords_from_tokens(tokens):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    return [each_token for each_token in tokens if each_token not in stop_words]\n",
    "\n",
    "\n",
    "def set_tokens_to_lowercase(tokens):\n",
    "    \n",
    "    return [each_token.lower() for each_token in tokens]\n",
    "\n",
    "def preprocess(pstr1):\n",
    "    \n",
    "     s=split_text_to_tokens(pstr1)\n",
    "     s=remove_non_alphabetic_tokens(s)\n",
    "     s=remove_punctuation_from_tokens(s)\n",
    "     s=set_tokens_to_lowercase(s)\n",
    "     return s\n",
    " \n",
    "##################\n",
    "\n",
    "raw_data = pd.read_csv(\"C:/IR Course/NLP_Intro/SMSSpamCollection.csv\",  encoding='iso-8859-1') \n",
    "\n",
    "raw_data[\"Email\"].value_counts().plot(kind = 'pie', explode = [0, 0.1], figsize = (6, 6), autopct = '%1.1f%%', shadow = True)\n",
    "plt.ylabel(\"Spam vs Ham\")\n",
    "plt.legend([\"Ham\", \"Spam\"])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def getsampledata(pdf, psamp):\n",
    "    types = ['spam', 'ham']\n",
    "    allsamples = pd.DataFrame()\n",
    "    for i in types:\n",
    "        data1 = pdf[pdf.Email == i]\n",
    "        rows = np.random.choice(data1.index.values, psamp)\n",
    "        sampled_data = pdf.loc[rows] \n",
    "        allsamples = allsamples.append(sampled_data, ignore_index=True)\n",
    "\n",
    "    return allsamples\n",
    "\n",
    "samp_data = getsampledata (raw_data, 10)\n",
    "\n",
    "def populatedictcorpus(data):\n",
    "    pdict1 = {} \n",
    "    textspam = \"\"\n",
    "    textham = \"\"\n",
    "    list_WtV_spam=[]\n",
    "    list_WtV_ham=[]\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "            if row['Email']=='spam':\n",
    "                                      #s= preprocess(row['Description'])\n",
    "                                      #textspam = textspam + \" \".join(s) \n",
    "                                      textspam = row['Description'] + \" \" +  textspam\n",
    "                                      list_WtV_spam.append(row['Description'].split(\" \")) \n",
    " \n",
    "            else:   \n",
    "                                      #s= preprocess(row['Description'])\n",
    "                                      #textham = textham + \" \".join(s) \n",
    "                                      textham = row['Description'] + \" \" +  textham\n",
    "                                      list_WtV_ham.append(row['Description'].split(\" \")) \n",
    "                                      \n",
    "                                      \n",
    "   \n",
    "    pdict1.update({'spam': textspam})\n",
    "    pdict1.update({'ham': textham})\n",
    "   \n",
    "    alldata = [pdict1,list_WtV_spam, list_WtV_ham ]\n",
    "   \n",
    "    return alldata\n",
    "   \n",
    "  \n",
    "def freqalltokens(palltext):\n",
    "        \n",
    "            dictcounts = {}\n",
    "            palltext = palltext.split(\" \")\n",
    "            for token in palltext:\n",
    "                if token in dictcounts:\n",
    "                    dictcounts[token] = dictcounts[token] + 1\n",
    "                else:\n",
    "                     dictcounts[token] =  1\n",
    "            sorted_val = sorted(dictcounts.items(), key=operator.itemgetter(1), reverse=True)         \n",
    "            return sorted_val          \n",
    "\n",
    "\n",
    "def plotall(px, py):\n",
    "\n",
    "    plt.xticks(fontsize=6, rotation=90)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.plot(px, py)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def percentoftotal (px, py, psum):\n",
    "    pfreqdict = {}\n",
    "    for i in range (10):\n",
    "         pfreqlist = []\n",
    "         t= py[i]/psum *100\n",
    "         pfreqlist.append(py[i])\n",
    "         pfreqlist.append(round(t, 3))\n",
    "         t = py[i]/psum * 1000\n",
    "         pfreqlist.append(round(t,3))\n",
    "         pfreqdict[px[i]] = pfreqlist\n",
    "    return pfreqdict\n",
    "\n",
    "####lexical diversity\n",
    "\n",
    "def lexical_diversity(text):\n",
    "  \n",
    "    info = []\n",
    "    info.append(len(text))\n",
    "    info.append(len(set(text))) \n",
    "    info.append(len(set(text))/len(text))\n",
    "    return info\n",
    "\n",
    "         \n",
    "count_spamham = []\n",
    "sum_tokens=0\n",
    "alltext = \"\"\n",
    "complete_list = populatedictcorpus(samp_data)\n",
    "dict1 = complete_list[0]\n",
    "for key in dict1:\n",
    "    count_spamham.append([key, len(dict1[key])])\n",
    "    sum_tokens=len(dict1[key]) + sum_tokens\n",
    "    alltext = alltext + dict1[key]\n",
    "\n",
    "a=freqalltokens(alltext)\n",
    "\n",
    "#x, y = zip(*freqalltokens(alltext))\n",
    "\n",
    "token = []\n",
    "count = []\n",
    "for item in a:\n",
    "    token.append(item[0])\n",
    "    count.append(item[1])\n",
    "    \n",
    " \n",
    "import csv\n",
    "\n",
    "with open(\"C:/IR Course/NLP_Intro/datafreq.csv\", mode='w', newline='', encoding='iso-8859-1') as datafreq:\n",
    "    datafreq = csv.writer(datafreq, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    datafreq.writerow(token)\n",
    "    datafreq.writerow(count)\n",
    "\n",
    " \n",
    "    \n",
    "    \n",
    "plotall(token, count)\n",
    "\n",
    "\n",
    "for itemnum in range (len(count_spamham)):\n",
    "    print (\"Number of tokens in:\", count_spamham[itemnum][0], count_spamham[itemnum][1])\n",
    "print (\"Number of tokens in text:\", sum_tokens)\n",
    "\n",
    "\n",
    "\n",
    "freqdict= percentoftotal (token, count, sum_tokens)\n",
    "\n",
    "##plot\n",
    "tokens = tuple(freqdict.keys())\n",
    "values = freqdict.values()\n",
    "total, percent,normalised = zip(*values)\n",
    "\n",
    "\n",
    "plotall(tokens, normalised)\n",
    "\n",
    "\n",
    "lingstats = lexical_diversity(dict1['spam'] + \" \" + dict1['ham'])\n",
    "print (\"Total tokens:\", lingstats[0], \"Total Unique Words:\", lingstats[1], \"Type/Token Ratio:\", round(lingstats[2], 6))\n",
    "\n",
    "\n",
    "#Spam Word cloud\n",
    "\n",
    "def words_to_cloud (pstr):\n",
    "    wordcloud = WordCloud().generate(pstr)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "words_to_cloud (\" \".join(dict1['spam'].split(\" \")))\n",
    "\n",
    "#dispersion\n",
    "#spam_text_tokens = nltk.word_tokenize(dict1['spam']) #tokenize\n",
    "spam_text_tokens = nltk.word_tokenize(dict1['spam']) #tokenize\n",
    "spam_text_object = nltk.Text(spam_text_tokens) #turning it into nltk.Text object to be able to use .condordance, .similar etc\n",
    "spam_text_object.dispersion_plot([\"call\", \"service\", \"text\"])\n",
    "\n",
    "###################\n",
    "   \n",
    "# get concordance\n",
    "\n",
    "allspamtokens = nltk.word_tokenize(dict1['spam']) #tokenize\n",
    "spamtoken_object = nltk.Text(allspamtokens) #turning it into nltk.Text object to be able to use .condordance, .similar etc\n",
    "spamtoken_object.concordance('call')\n",
    "\n",
    "################\n",
    "\n",
    "##Get bigrams\n",
    "\n",
    "#bigrm = nltk.bigrams(dict1['ham'].split(\" \"))\n",
    "#biagram_collocation = BigramCollocationFinder.from_words(dict1['ham'].split(\" \")) \n",
    "#biagram_collocation.nbest(BigramAssocMeasures.likelihood_ratio, 15) \n",
    "\n",
    "\n",
    "def generate_collocations(tokens):\n",
    "    '''\n",
    "    Given list of tokens, return collocations.\n",
    "    '''\n",
    "\n",
    "    ignored_words = nltk.corpus.stopwords.words('english')\n",
    "    bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
    "    bigramFinder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
    "    bigram_freq = bigramFinder.ngram_fd.items()\n",
    "    bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n",
    "   \n",
    "    return bigramFreqTable\n",
    "\n",
    "print (generate_collocations(dict1['spam'].split()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###plot embeddings\n",
    "complete_list = populatedictcorpus(raw_data)\n",
    "#spam\n",
    "model = Word2Vec(complete_list[1], min_count=20,size=50,workers=4)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "# access vector for one word\n",
    "print(model['win'])\n",
    "# save model\n",
    "model.save('model.bin')\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)\n",
    "\n",
    "#plot with PCA\n",
    "\n",
    "#ham\n",
    "\n",
    "model2 = Word2Vec(complete_list[2], min_count=20,size=50,workers=4)\n",
    "# summarize the loaded model\n",
    "print(model2)\n",
    "# summarize vocabulary\n",
    "words2 = list(model2.wv.vocab)\n",
    "print(words2)\n",
    "# access vector for one word\n",
    "print(model2['guy'])\n",
    "# save model\n",
    "model2.save('model2.bin')\n",
    "# load model\n",
    "new_model2 = Word2Vec.load('model2.bin')\n",
    "print(new_model2)\n",
    "\n",
    "# dimensionality reduction \n",
    "X = model[model.wv.vocab]\n",
    "X2 = model2[model2.wv.vocab]\n",
    "\n",
    "\n",
    "pca1 = PCA(n_components=2)\n",
    "result = pca1.fit_transform(X)\n",
    "\n",
    "pca2 = PCA(n_components=2)\n",
    "result2 = pca2.fit_transform(X2)\n",
    "\n",
    "# Create plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(result[:, 0], result[:, 1], c=\"red\",s=5,label=\"spam\")\n",
    "ax.scatter(result2[:, 0], result2[:, 1], c=\"blue\",s=5,label=\"ham\")\n",
    "plt.xlim(-0.50, 1.25) \n",
    "plt.ylim(-0.04, 0.04)\n",
    "plt.gcf().set_size_inches((10, 10))   \n",
    "\n",
    "\n",
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "\tplt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "\n",
    "\n",
    "words2 = list(model2.wv.vocab)\n",
    "for i, word2 in enumerate(words2):\n",
    "\tplt.annotate(word2, xy=(result2[i, 0], result2[i, 1]))\n",
    "\n",
    "\n",
    "plt.title('Spam Ham Embeddings')\n",
    "plt.legend(loc=2)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "##separate\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4), sharey=True, dpi=120)\n",
    "\n",
    "# Plot\n",
    "ax1.scatter(result[:, 0], result[:, 1], c=\"red\",label=\"spam\", s= 5)\n",
    "\n",
    "ax2.scatter(result2[:, 0], result2[:, 1], c=\"blue\",label=\"ham\", s= 5)\n",
    "\n",
    "# Title, X and Y labels, X and Y Lim\n",
    "ax1.set_title('Spam Embeddings'); ax2.set_title('Ham Embeddings')\n",
    "ax1.set_xlabel('X');  ax2.set_xlabel('X')  # x label\n",
    "ax1.set_ylabel('Y');  ax2.set_ylabel('Y')  # y label\n",
    "ax1.set_xlim(-0.50, 1.25) ;  ax2.set_xlim(-0.50, 1.25)   # x axis limits\n",
    "ax1.set_ylim(-0.04, 0.04);  ax2.set_ylim(-0.04, 0.04)  # y axis limits\n",
    "\n",
    "\n",
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "\tax1.annotate(word, xy=(result[i, 0], result[i, 1]), fontsize=5)\n",
    "\n",
    "\n",
    "words2 = list(model2.wv.vocab)\n",
    "for i, word2 in enumerate(words2):\n",
    "\tax2.annotate(word2, xy=(result2[i, 0], result2[i, 1]), fontsize=5)\n",
    "\n",
    "\n",
    "ax1.legend(loc=2)\n",
    "ax2.legend(loc=5)\n",
    "\n",
    "\n",
    "# ax2.yaxis.set_ticks_position('none') \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
